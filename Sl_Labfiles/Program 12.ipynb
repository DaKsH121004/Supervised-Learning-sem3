
# prepare a model for prediction of survival from Titanic, escape using a random forest and compare the accuracy with other classyfiers  alsoimport pandas as pdfrom sklearn.model_selection import train_test_splitfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics import accuracy_score , classification_reportimport warningswarnings.filterwarnings('ignore')from sklearn.preprocessing import LabelEncoder

 
[ ]
df = pd.read_csv('Titanic-Dataset.csv')

 
[ ]
df.head()

 
 
[ ]
df.info()

 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
 
[ ]
 df.shape
 (891, 12)
 
[ ]
 # dropping those rows where target variable is missing
df.dropna(subset=['Survived'])
 
 
[ ]
 df.info()
 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
 
[ ]
 x = df[['Pclass','Sex','Age','SibSp','Parch','Fare']]
y = df['Survived']

 
[ ]
 # label encoding
le = LabelEncoder()
x['Sex'] = le.fit_transform(x['Sex'])
 
[ ]
 x['Age'] = x['Age'].fillna(x['Age'].mean())
 
[ ]
 x['Age']
 
 
[ ]
 x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)
 
[ ]
 # create a random forest classifier
model = RandomForestClassifier(n_estimators=100,random_state=42)
 
[ ]
 # Train the Classifier
model.fit(x_train,y_train)
 
 
[ ]
 y_pred = model.predict(x_test)
 
[ ]
 # evaluate the model
accuracy = accuracy_score(y_test,y_pred)
classification_rep = classification_report(y_test,y_pred)
 
[ ]
 print(accuracy)
 0.8156424581005587
 
[ ]
 print(classification_rep)
               precision    recall  f1-score   support

           0       0.82      0.88      0.85       105
           1       0.81      0.73      0.77        74

    accuracy                           0.82       179
   macro avg       0.81      0.80      0.81       179
weighted avg       0.82      0.82      0.81       179

 
[ ]
 # compare with another models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
model11=KNeighborsClassifier(n_neighbors=9)
model12=GaussianNB()
model13=DecisionTreeClassifier(criterion='entropy')
model14=RandomForestClassifier(n_estimators=100)
modellist=[model11,model12,model13,model14]
 
[ ]
 model11
 
 
[ ]
 model12
 
 
[ ]
 model13
 
 
[ ]
 model14
 
 
[ ]
 from sklearn.metrics import confusion_matrix,classification_report,classification_report


for i in modellist:
    i.fit(x_train,y_train)
    y_pred = i.predict(x_test)
    print('the classification details of model ', i ,'is below')
    print('the confusion matrix of  ', i ,'is')
    print(confusion_matrix(y_test,y_pred))
    print('accuracy score of' , i ,'is')
    print(accuracy_score(y_test,y_pred))
    print('classification report of' , i ,'is')
    print(classification_report(y_test,y_pred))
 the classification details of model  KNeighborsClassifier(n_neighbors=9) is below
the confusion matrix of   KNeighborsClassifier(n_neighbors=9) is
[[85 20]
 [34 40]]
accuracy score of KNeighborsClassifier(n_neighbors=9) is
0.6983240223463687
classification report of KNeighborsClassifier(n_neighbors=9) is
              precision    recall  f1-score   support

           0       0.71      0.81      0.76       105
           1       0.67      0.54      0.60        74

    accuracy                           0.70       179
   macro avg       0.69      0.68      0.68       179
weighted avg       0.69      0.70      0.69       179

the classification details of model  GaussianNB() is below
the confusion matrix of   GaussianNB() is
[[85 20]
 [21 53]]
accuracy score of GaussianNB() is
0.770949720670391
classification report of GaussianNB() is
              precision    recall  f1-score   support

           0       0.80      0.81      0.81       105
           1       0.73      0.72      0.72        74

    accuracy                           0.77       179
   macro avg       0.76      0.76      0.76       179
weighted avg       0.77      0.77      0.77       179

the classification details of model  DecisionTreeClassifier(criterion='entropy') is below
the confusion matrix of   DecisionTreeClassifier(criterion='entropy') is
[[85 20]
 [20 54]]
accuracy score of DecisionTreeClassifier(criterion='entropy') is
0.776536312849162
classification report of DecisionTreeClassifier(criterion='entropy') is
              precision    recall  f1-score   support

           0       0.81      0.81      0.81       105
           1       0.73      0.73      0.73        74

    accuracy                           0.78       179
   macro avg       0.77      0.77      0.77       179
weighted avg       0.78      0.78      0.78       179

the classification details of model  RandomForestClassifier() is below
the confusion matrix of   RandomForestClassifier() is
[[90 15]
 [21 53]]
accuracy score of RandomForestClassifier() is
0.7988826815642458
classification report of RandomForestClassifier() is
              precision    recall  f1-score   support

           0       0.81      0.86      0.83       105
           1       0.78      0.72      0.75        74

    accuracy                           0.80       179
   macro avg       0.80      0.79      0.79       179
weighted avg       0.80      0.80      0.80       179

 
[ ]

Start coding or generate with AI.
Colab paid products - Cancel contracts here
